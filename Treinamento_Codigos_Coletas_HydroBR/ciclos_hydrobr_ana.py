# -*- coding: utf-8 -*-
"""Ciclos_HydroBR_ANA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uXHHoi8Vt-tEFd6yCk_zONmdHvLrKuqB

# üìä Projeto de Ci√™ncia de Dados

# Modelagem, Detec√ß√£o e Predi√ß√£o dos Ciclos Hidrol√≥gicos do Pantanal

---

## üåé 1. Contexto

O **Pantanal** √© a maior plan√≠cie alag√°vel tropical do mundo, com din√¢mica hidrol√≥gica altamente sazonal. A altern√¢ncia entre cheias e secas impacta biodiversidade, economia, navega√ß√£o, agricultura e seguran√ßa h√≠drica.

Este projeto visa modelar quantitativamente esses ciclos e investigar poss√≠veis mudan√ßas estruturais ao longo das √∫ltimas d√©cadas.

---

## üéØ 2. Problema de Pesquisa

> **Qual √© a din√¢mica, frequ√™ncia, intensidade, espacializa√ß√£o e tend√™ncia dos ciclos de cheias e secas no Pantanal ao longo das √∫ltimas d√©cadas (anual, mensal e di√°ria)?**

### Quest√µes complementares:

* √â poss√≠vel detectar mudan√ßas estruturais no regime hidrol√≥gico?
* H√° intensifica√ß√£o de eventos extremos?
* Podemos prever eventos futuros com modelos estat√≠sticos e Deep Learning?
* Podemos segmentar os padr√µes por munic√≠pio?

---

## üóÇÔ∏è 3. Fonte de Dados

Os dados ser√£o obtidos via pacote **HydroBR**, que fornece acesso estruturado √†s bases da **Ag√™ncia Nacional de √Åguas (ANA)**.

### Vari√°veis dispon√≠veis:

* Cota (n√≠vel do rio)
* Vaz√£o
* Precipita√ß√£o
* Metadados das esta√ß√µes

---

## üó∫Ô∏è 4. Munic√≠pios do Pantanal Considerados

### üìç Mato Grosso (MT) ‚Äî 7 munic√≠pios

* Bar√£o de Melga√ßo (MT)
* C√°ceres (MT)
* Lambari d‚ÄôOeste (MT)
* Pocon√© (MT)
* Nossa Senhora do Livramento (MT)
* Santo Ant√¥nio do Leverger (MT)
* Itiquira (MT)

---

### üìç Mato Grosso do Sul (MS) ‚Äî 9 munic√≠pios

* Corumb√° (MS) ‚Äî ‚ÄúCapital do Pantanal‚Äù
* Lad√°rio (MS)
* Aquidauana (MS)
* Miranda (MS)
* Bodoquena (MS)
* Porto Murtinho (MS)
* Rio Verde de Mato Grosso (MS)
* Sonora (MS)
* Coxim (MS)

---

### üìä Total

* **16 munic√≠pios**
* **7 em MT**
* **9 em MS**

Munic√≠pios com maior √°rea inserida no Pantanal:

* **Bar√£o de Melga√ßo (MT)**
* **Corumb√° (MS)**

---

# üß† 5. Arquitetura Anal√≠tica do Projeto

---

## 5.1 Coleta Automatizada (HydroBR)

### ‚úî Sele√ß√£o de esta√ß√µes

* Estados: MT e MS
* Filtro por esta√ß√µes com longas s√©ries hist√≥ricas
* Download automatizado

### ‚úî Consolida√ß√£o

* DataFrame unificado
* Index temporal padronizado

---

## 5.2 Pr√©-processamento

### üîπ Tratamento de Dados

* Interpola√ß√£o temporal
* Remo√ß√£o de outliers (IQR / Z-score)
* Verifica√ß√£o de consist√™ncia temporal

### üîπ Engenharia de Vari√°veis

* Anomalias hidrol√≥gicas
* M√©dias m√≥veis
* √çndice padronizado (z-score)
* Classifica√ß√£o autom√°tica:

  * Cheia extrema
  * Cheia moderada
  * Normal
  * Seca
  * Seca extrema

---

## 5.3 An√°lise Explorat√≥ria (EDA)

### üìä Estat√≠sticas Descritivas

* M√©dia
* Desvio padr√£o
* Assimetria
* Quantis extremos

### üìà Visualiza√ß√£o Temporal

* S√©ries hist√≥ricas completas
* Zoom por d√©cadas
* Heatmap anual

### üîÑ Autocorrela√ß√£o

* ACF
* PACF

### üìâ Decomposi√ß√£o

STL Decomposition:

* Tend√™ncia
* Sazonalidade
* Res√≠duo

---

## 5.4 Extra√ß√£o de Ciclos ‚Äì An√°lise Espectral

### üéµ Transformada de Fourier (FFT)

Aplica√ß√£o:

[
FFT(x_t)
]

### Objetivos:

* Detectar periodicidades dominantes (ex: 12 meses)
* Identificar ciclos interanuais
* Avaliar mudan√ßas na frequ√™ncia dominante ao longo das d√©cadas

### T√©cnicas:

* Periodograma
* Densidade espectral
* Filtro passa-banda
* Wavelet (opcional avan√ßado)

---

## 5.5 Modelagem de S√©ries Temporais

### üìå Modelos Estat√≠sticos

* ARIMA
* SARIMA
* SARIMAX (precipita√ß√£o como vari√°vel ex√≥gena)

### Avalia√ß√£o:

* AIC
* BIC
* RMSE
* Cross-validation temporal

---

### ü§ñ Modelos de Deep Learning

* LSTM
* Transformer Temporal (Time Series Transformer)

### Objetivos:

* Previs√£o de cheias futuras
* Antecipa√ß√£o de eventos extremos
* Compara√ß√£o com modelos cl√°ssicos

---

## 5.6 Detec√ß√£o de Eventos Extremos

### Crit√©rios:

* Percentil 90‚Äì95 ‚Üí Cheia
* Percentil 5‚Äì10 ‚Üí Seca
* Dura√ß√£o m√≠nima consecutiva
* Intensidade acumulada

### Visualiza√ß√µes:

* Gr√°fico tipo Gantt
* Curvas de severidade
* Ranking de eventos hist√≥ricos

---

## 5.7 An√°lise de Tend√™ncia

* Teste de Mann-Kendall
* Regress√£o linear robusta
* Theil-Sen
* Change Point Detection

### Objetivos:

* Detectar intensifica√ß√£o de secas
* Avaliar aumento da variabilidade
* Identificar altera√ß√µes no regime sazonal

---

## 5.8 Visualiza√ß√£o Interativa

### üìç Mapas

* Mapa interativo das esta√ß√µes
* Classifica√ß√£o por intensidade
* Timeline animada

### Ferramentas:

* Folium
* Plotly
* GeoPandas

---

## 5.9 Exporta√ß√£o

* Exporta√ß√£o autom√°tica em Excel (.xlsx)
* CSV
* Relat√≥rio PDF
* Download direto no Colab

---

# üìÇ 6. Estrutura do Notebook no Google Colab

```
01 - Instala√ß√£o de Pacotes
02 - Coleta de Dados (HydroBR)
03 - Pr√©-processamento
04 - EDA
05 - An√°lise Espectral (FFT)
06 - Modelagem SARIMA
07 - Modelagem LSTM / Transformer
08 - Detec√ß√£o de Eventos Extremos
09 - Mapas Interativos
10 - Exporta√ß√£o de Resultados
```

---

# üìà 7. Resultados Esperados

* ‚úî Periodicidade dominante dos ciclos
* ‚úî Identifica√ß√£o de ciclos interanuais
* ‚úî Quantifica√ß√£o da dura√ß√£o m√©dia das cheias
* ‚úî Tend√™ncia estatisticamente significativa (ou n√£o)
* ‚úî Modelo preditivo funcional
* ‚úî Dashboard interativo
* ‚úî Base export√°vel para an√°lise externa

---

# üî¨ 8. Diferenciais Cient√≠ficos

* Integra√ß√£o entre hidrologia e ci√™ncia de dados
* Uso combinado de FFT + SARIMA + Transformers
* Detec√ß√£o autom√°tica de eventos extremos
* An√°lise multiescalar (dias, meses, anos)
* Pipeline totalmente reprodut√≠vel

---

# üìö 9. Refer√™ncias T√©cnicas

* HydroBR (GitHub)
* Box & Jenkins ‚Äî Modelagem de S√©ries Temporais
* An√°lise Espectral em Hidrologia
* Deep Learning aplicado a s√©ries temporais
"""

# 01 ‚Äî Instala√ß√£o de Pacotes

!pip install hydrobr
!pip install statsmodels
!pip install pmdarima
!pip install ruptures
!pip install pymannkendall
!pip install plotly
!pip install folium
!pip install geopandas
!pip install openpyxl
!pip install tensorflow

# ================================================================
# Projeto Pantanal - Pipeline Integrado (PLU + FLU)
# ================================================================

# 0) Instala√ß√£o de pacotes
# !pip install -q hydrobr pymannkendall statsmodels openpyxl tqdm seaborn

# 1) Imports
import os, unicodedata, warnings, time
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm

import pymannkendall as mk
from statsmodels.tsa.seasonal import STL

import hydrobr
from hydrobr import get_data

plt.style.use("seaborn-v0_8-whitegrid")

# 2) Diret√≥rios de sa√≠da
OUTDIR_PLU = "/content/pantanal_output"
OUTDIR_FLU = "/content/pantanal_flu_output"
os.makedirs(OUTDIR_PLU, exist_ok=True)
os.makedirs(OUTDIR_FLU, exist_ok=True)

# 3) Lista dos munic√≠pios pantaneiros
municipios_pantanal = [
    "BAR√ÉO DE MELGA√áO","C√ÅCERES","LAMBARI D‚ÄôOESTE","POCON√â",
    "NOSSA SENHORA DO LIVRAMENTO","SANTO ANT√îNIO DO LEVERGER","ITIQUIRA",
    "CORUMB√Å","LAD√ÅRIO","AQUIDAUANA","MIRANDA","BODOQUENA",
    "PORTO MURTINHO","RIO VERDE DE MATO GROSSO","SONORA","COXIM"
]

def normalize_txt(s):
    if pd.isna(s): return s
    s = str(s).upper()
    s = unicodedata.normalize('NFKD', s).encode('ascii', errors='ignore').decode('utf-8')
    return s

municipios_norm = [normalize_txt(m) for m in municipios_pantanal]

# BLOCO 1 - PRECIPITA√á√ÉO (PLU)

print("üì• Baixando invent√°rio de esta√ß√µes PLU...")
inv_plu = get_data.ANA.list_prec_stations(source='ANAF')
inv_plu["City_norm"] = inv_plu["City"].apply(normalize_txt)
plu_pantanal = inv_plu[ inv_plu["City_norm"].isin(municipios_norm) ].copy()
plu_pantanal.to_csv(os.path.join(OUTDIR_PLU, "inventario_PLU_pantanal.csv"), index=False)

codes_plu = plu_pantanal["Code"].astype(str).unique().tolist()

def baixar_prec_wide(codigos, chunk_size=30, pausa=0.3):
    df_all = pd.DataFrame()
    for i in tqdm(range(0, len(codigos), chunk_size), desc="Baixando PLU"):
        bloco = codigos[i:i+chunk_size]
        try:
            df_chunk = get_data.ANA.prec_data(bloco, only_consisted=False)
            df_chunk.index = pd.to_datetime(df_chunk.index, errors="coerce")
            df_chunk = df_chunk[~df_chunk.index.isna()].sort_index()
            df_all = pd.concat([df_all, df_chunk], axis=1)
            time.sleep(pausa)
        except Exception as e:
            print(f"[AVISO] Falha no lote {bloco[:3]}...: {e}")
    return df_all

df_plu = baixar_prec_wide(codes_plu)

def analisar_prec(serie):
    serie = pd.to_numeric(serie, errors="coerce").asfreq("D")
    mensal = serie.resample("M").sum(min_count=1).dropna()
    if mensal.shape[0] < 24: return None
    stl = STL(mensal, period=12, robust=True).fit()
    mkres = mk.original_test(mensal.values)
    return {"inicio":str(mensal.index.min().date()),"fim":str(mensal.index.max().date()),
            "n_obs_mensal":mensal.shape[0],"tendencia_MK":mkres.trend,
            "p_valor_MK":mkres.p,"slope_sen":getattr(mkres,"slope",None)}

metricas_plu = []
for cod in tqdm(df_plu.columns, desc="Analisando PLU"):
    met = analisar_prec(df_plu[cod])
    if met: met.update({"codigo":cod,"tipo":"PLU"}); metricas_plu.append(met)


# BLOCO 2 - FLUVIOMETRIA (VAZ√ÉO + N√çVEL)

print("üì• Baixando invent√°rio de esta√ß√µes FLU...")
inv_flu = get_data.ANA.list_flow_stations(source='ANAF')
inv_flu["City_norm"] = inv_flu["City"].apply(normalize_txt)
flu_pantanal = inv_flu[ inv_flu["City_norm"].isin(municipios_norm) ].copy()
flu_pantanal.to_csv(os.path.join(OUTDIR_FLU, "inventario_FLU_pantanal.csv"), index=False)

codes_flu = flu_pantanal["Code"].astype(str).unique().tolist()

def baixar_flu(codigos, tipo="flow", chunk_size=30, pausa=0.3):
    fn = get_data.ANA.flow_data if tipo=="flow" else get_data.ANA.stage_data
    df_all = pd.DataFrame()
    for i in tqdm(range(0, len(codigos), chunk_size), desc=f"Baixando {tipo.upper()}"):
        bloco = codigos[i:i+chunk_size]
        try:
            df_chunk = fn(bloco, only_consisted=False)
            df_chunk.index = pd.to_datetime(df_chunk.index, errors="coerce")
            df_chunk = df_chunk[~df_chunk.index.isna()].sort_index()
            df_all = pd.concat([df_all, df_chunk], axis=1)
            time.sleep(pausa)
        except Exception as e:
            print(f"[AVISO] Falha no lote {bloco[:3]}...: {e}")
    return df_all

df_flow = baixar_flu(codes_flu, tipo="flow")
df_stage = baixar_flu(codes_flu, tipo="stage")

def analisar_flu(serie):
    serie = pd.to_numeric(serie, errors="coerce").asfreq("D")
    mensal = serie.resample("M").mean().dropna()
    if mensal.shape[0] < 24: return None
    stl = STL(mensal, period=12, robust=True).fit()
    mkres = mk.original_test(mensal.values)
    return {"inicio":str(mensal.index.min().date()),"fim":str(mensal.index.max().date()),
            "n_obs_mensal":mensal.shape[0],"tendencia_MK":mkres.trend,
            "p_valor_MK":mkres.p,"slope_sen":getattr(mkres,"slope",None)}

metricas_flu = []
for cod in tqdm(df_flow.columns, desc="Analisando VAZ√ÉO"):
    met = analisar_flu(df_flow[cod])
    if met: met.update({"codigo":cod,"tipo":"VAZAO"}); metricas_flu.append(met)

for cod in tqdm(df_stage.columns, desc="Analisando N√çVEL"):
    met = analisar_flu(df_stage[cod])
    if met: met.update({"codigo":cod,"tipo":"NIVEL"}); metricas_flu.append(met)


# BLOCO 3 - CONSOLIDA√á√ÉO FINAL

df_resumo = pd.DataFrame(metricas_plu + metricas_flu)
xls_out = "/content/Resumo_MK_STL_Pantanal.xlsx"
df_resumo.to_excel(xls_out, index=False, engine="openpyxl")
print(f"‚úÖ Relat√≥rio consolidado salvo: {xls_out}")

import pandas as pd

# Carrega o arquivo gerado
xls_out = "/content/Resumo_MK_STL_Pantanal.xlsx"
df_resumo = pd.read_excel(xls_out)

# Mostra as primeiras linhas
df_resumo.head()

# Quantidade de s√©rie analisadas
df_resumo["tipo"].value_counts()

# Filtrar s√≥ tend√™ncias decrescentes (ex.: queda de vaz√£o):
df_resumo[ df_resumo["tendencia_MK"] == "decreasing" ]

# Tend√™ncias de Crescimento
df_resumo[ df_resumo["tendencia_MK"] == "increasing" ]

# Sem Tend√™ncias
df_resumo[ df_resumo["tendencia_MK"] == "no trend" ]

# Resumo estat√≠stico dos slopes (inclina√ß√£o Sen):
df_resumo.groupby("tipo")["slope_sen"].describe()

# Visualizar distribui√ß√£o de p‚Äëvalores:
import matplotlib.pyplot as plt

plt.hist(df_resumo["p_valor_MK"], bins=20, color="skyblue", edgecolor="black")
plt.title("Distribui√ß√£o dos p-valores (Mann-Kendall)")
plt.xlabel("p-valor")
plt.ylabel("Frequ√™ncia")
plt.show()

# Abrir e visualizar os resultados do Resumo_MK_STL_Pantanal.xlsx com alguns gr√°ficos de tend√™ncia por munic√≠pio.
import pandas as pd
import matplotlib.pyplot as plt

# Carrega o arquivo consolidado
xls_out = "/content/Resumo_MK_STL_Pantanal.xlsx"
df_resumo = pd.read_excel(xls_out)

# Mostra as primeiras linhas
df_resumo.head()

# Gr√°fico de tend√™ncias por munic√≠pio
# Aqui vamos separar por tipo (PLU = chuva, VAZ√ÉO, N√çVEL) e mostrar a inclina√ß√£o Sen (slope).
import seaborn as sns

plt.figure(figsize=(12,6))
sns.barplot(data=df_resumo, x="codigo", y="slope_sen", hue="tipo")
plt.xticks(rotation=90)
plt.title("Inclina√ß√£o Sen por esta√ß√£o (chuva, vaz√£o e n√≠vel)")
plt.ylabel("Slope Sen (tend√™ncia)")
plt.xlabel("C√≥digo da esta√ß√£o")
plt.legend(title="Tipo")
plt.show()

# Tend√™ncia por munic√≠pio
# Se quiser agrupar por cidade:

# Junta invent√°rio com resultados
df_resumo["municipio"] = df_resumo["codigo"].map(lambda x: str(x))  # aqui voc√™ pode cruzar com invent√°rio PLU/FLU

plt.figure(figsize=(12,6))
sns.countplot(data=df_resumo, x="municipio", hue="tendencia_MK")
plt.xticks(rotation=90)
plt.title("Tend√™ncia Mann-Kendall por munic√≠pio")
plt.ylabel("N√∫mero de s√©ries")
plt.xlabel("Munic√≠pio")
plt.legend(title="Tend√™ncia")
plt.show()

# visualiza√ß√£o espacial das esta√ß√µes.
# Mapa interativo do Pantanal.
# Ao clicar no ponto, aparece munic√≠pio, tipo (chuva, vaz√£o, n√≠vel) e tend√™ncia.
# Cada esta√ß√£o marcada com uma cor:
# Verde ‚Üí tend√™ncia crescente
# Vermelho ‚Üí tend√™ncia decrescente
# Azul ‚Üí est√°vel

!pip install folium

import folium
import pandas as pd

# Carrega o resumo
xls_out = "/content/Resumo_MK_STL_Pantanal.xlsx"
df_resumo = pd.read_excel(xls_out)

# Exemplo: se o invent√°rio PLU/FLU tiver colunas de latitude/longitude
# (normalmente "Lat" e "Lon"), podemos juntar com o resumo
# Aqui vou supor que voc√™ j√° tem invent√°rio salvo em CSV
inv_plu = pd.read_csv("/content/pantanal_output/inventario_PLU_pantanal.csv")
inv_flu = pd.read_csv("/content/pantanal_flu_output/inventario_FLU_pantanal.csv")

# Junta invent√°rios
inv_total = pd.concat([inv_plu, inv_flu], ignore_index=True)

# Normaliza c√≥digo para cruzar com df_resumo
inv_total["Code"] = inv_total["Code"].astype(str)
df_resumo["codigo"] = df_resumo["codigo"].astype(str)

# Faz merge para trazer lat/lon
df_mapa = df_resumo.merge(inv_total, left_on="codigo", right_on="Code", how="left")

# Cria mapa centralizado no Pantanal
mapa = folium.Map(location=[-17.5, -57.0], zoom_start=6)

# Adiciona marcadores
for _, row in df_mapa.iterrows():
    if pd.notna(row.get("Lat")) and pd.notna(row.get("Lon")):
        cor = "green" if row["tendencia_MK"]=="increasing" else "red" if row["tendencia_MK"]=="decreasing" else "blue"
        folium.CircleMarker(
            location=[row["Lat"], row["Lon"]],
            radius=6,
            color=cor,
            fill=True,
            fill_color=cor,
            popup=f"{row['City']} - {row['tipo']} ({row['tendencia_MK']})"
        ).add_to(mapa)

mapa

# Exporta o mapa para HTML
mapa.save("/content/mapa_pantanal.html")

# Exporta o mapa para HTML
from google.colab import files
files.download("/content/mapa_pantanal.html")

# Gr√°fico de tend√™ncias por munic√≠pio (usando nomes das cidades)
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12,6))
sns.barplot(data=df_mapa, x="City", y="slope_sen", hue="tipo")
plt.xticks(rotation=90)
plt.title("Inclina√ß√£o Sen por munic√≠pio (chuva, vaz√£o e n√≠vel)")
plt.ylabel("Slope Sen (tend√™ncia)")
plt.xlabel("Munic√≠pio")
plt.legend(title="Tipo")
plt.show()

# Tend√™ncia por munic√≠pio (Mann-Kendall)
plt.figure(figsize=(12,6))
sns.countplot(data=df_mapa, x="City", hue="tendencia_MK")
plt.xticks(rotation=90)
plt.title("Tend√™ncia Mann-Kendall por munic√≠pio")
plt.ylabel("N√∫mero de s√©ries")
plt.xlabel("Munic√≠pio")
plt.legend(title="Tend√™ncia")
plt.show()

# Inclina√ß√£o Sen por munic√≠pio (ordenado pelo slope)
import seaborn as sns
import matplotlib.pyplot as plt

# Ordena os munic√≠pios pela m√©dia do slope
ordem_municipios = (
    df_mapa.groupby("City")["slope_sen"]
    .mean()
    .sort_values()
    .index
)

plt.figure(figsize=(12,6))
sns.barplot(
    data=df_mapa,
    x="City",
    y="slope_sen",
    hue="tipo",
    order=ordem_municipios
)
plt.xticks(rotation=90)
plt.title("Inclina√ß√£o Sen por munic√≠pio (chuva, vaz√£o e n√≠vel)")
plt.ylabel("Slope Sen (tend√™ncia)")
plt.xlabel("Munic√≠pio")
plt.legend(title="Tipo")
plt.show()

# Tend√™ncia Mann-Kendall por munic√≠pio (ordenado pela contagem)
# Ordena os munic√≠pios pela quantidade de s√©ries
ordem_municipios = (
    df_mapa["City"]
    .value_counts()
    .index
)

plt.figure(figsize=(12,6))
sns.countplot(
    data=df_mapa,
    x="City",
    hue="tendencia_MK",
    order=ordem_municipios
)
plt.xticks(rotation=90)
plt.title("Tend√™ncia Mann-Kendall por munic√≠pio")
plt.ylabel("N√∫mero de s√©ries")
plt.xlabel("Munic√≠pio")
plt.legend(title="Tend√™ncia")
plt.show()

# Inclina√ß√£o Sen por munic√≠pio (horizontal e ordenado)
import seaborn as sns
import matplotlib.pyplot as plt

# Ordena os munic√≠pios pela m√©dia do slope
ordem_municipios = (
    df_mapa.groupby("City")["slope_sen"]
    .mean()
    .sort_values()
    .index
)

plt.figure(figsize=(10,8))
sns.barplot(
    data=df_mapa,
    y="City",
    x="slope_sen",
    hue="tipo",
    order=ordem_municipios,
    orient="h"
)
plt.title("Inclina√ß√£o Sen por munic√≠pio (chuva, vaz√£o e n√≠vel)")
plt.xlabel("Slope Sen (tend√™ncia)")
plt.ylabel("Munic√≠pio")
plt.legend(title="Tipo", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.show()

# Tend√™ncia Mann-Kendall por munic√≠pio (horizontal e ordenado)
# Ordena os munic√≠pios pela quantidade de s√©ries
ordem_municipios = (
    df_mapa["City"]
    .value_counts()
    .index
)

plt.figure(figsize=(10,8))
sns.countplot(
    data=df_mapa,
    y="City",
    hue="tendencia_MK",
    order=ordem_municipios,
    orient="h"
)
plt.title("Tend√™ncia Mann-Kendall por munic√≠pio")
plt.xlabel("N√∫mero de s√©ries")
plt.ylabel("Munic√≠pio")
plt.legend(title="Tend√™ncia", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.show()

# Tend√™ncia Mann-Kendall por munic√≠pio (gr√°fico empilhado)
import matplotlib.pyplot as plt

# Conta tend√™ncias por munic√≠pio
df_tend = df_mapa.groupby(["City", "tendencia_MK"]).size().unstack(fill_value=0)

# Ordena munic√≠pios pela soma total de s√©ries
df_tend = df_tend.loc[df_tend.sum(axis=1).sort_values(ascending=False).index]

# Plota gr√°fico empilhado
df_tend.plot(kind="barh", stacked=True, figsize=(10,8), colormap="Set2")

plt.title("Tend√™ncia Mann-Kendall por munic√≠pio")
plt.xlabel("N√∫mero de s√©ries")
plt.ylabel("Munic√≠pio")
plt.legend(title="Tend√™ncia", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.show()

# Tend√™ncia Mann-Kendall por munic√≠pio (empilhado em porcentagem)
import matplotlib.pyplot as plt

# Conta tend√™ncias por munic√≠pio
df_tend = df_mapa.groupby(["City", "tendencia_MK"]).size().unstack(fill_value=0)

# Normaliza para porcentagem (cada munic√≠pio = 100%)
df_tend_percent = df_tend.div(df_tend.sum(axis=1), axis=0) * 100

# Ordena munic√≠pios pela soma total de s√©ries (ou pela % de "increasing", se preferir)
df_tend_percent = df_tend_percent.loc[df_tend.sum(axis=1).sort_values(ascending=False).index]

# Plota gr√°fico empilhado em porcentagem
df_tend_percent.plot(kind="barh", stacked=True, figsize=(10,8), colormap="Set2")

plt.title("Propor√ß√£o da tend√™ncia Mann-Kendall por munic√≠pio")
plt.xlabel("Propor√ß√£o (%)")
plt.ylabel("Munic√≠pio")
plt.legend(title="Tend√™ncia", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.show()

# Heatmap de tend√™ncias Mann-Kendall por munic√≠pio
import seaborn as sns
import matplotlib.pyplot as plt

# Conta tend√™ncias por munic√≠pio
df_tend = df_mapa.groupby(["City", "tendencia_MK"]).size().unstack(fill_value=0)

# Normaliza para porcentagem (cada munic√≠pio = 100%)
df_tend_percent = df_tend.div(df_tend.sum(axis=1), axis=0) * 100

# Ordena munic√≠pios pela soma total de s√©ries
df_tend_percent = df_tend_percent.loc[df_tend.sum(axis=1).sort_values(ascending=False).index]

# Cria heatmap
plt.figure(figsize=(12,8))
sns.heatmap(
    df_tend_percent,
    annot=True, fmt=".1f", cmap="YlGnBu",
    cbar_kws={'label': 'Propor√ß√£o (%)'}
)

plt.title("Mapa de calor das tend√™ncias Mann-Kendall por munic√≠pio")
plt.xlabel("Tend√™ncia")
plt.ylabel("Munic√≠pio")
plt.show()

# Cluster Heatmap de tend√™ncias Mann-Kendall por munic√≠pio
# Agrupar automaticamente os munic√≠pios com padr√µes de tend√™ncia semelhantes, criando dendrogramas que mostram quais cidades
# t√™m distribui√ß√µes parecidas de "increasing", "decreasing" e "no trend". Isso ajuda a identificar blocos de munic√≠pios com comportamento similar.


import seaborn as sns
import matplotlib.pyplot as plt

# Conta tend√™ncias por munic√≠pio
df_tend = df_mapa.groupby(["City", "tendencia_MK"]).size().unstack(fill_value=0)

# Normaliza para porcentagem (cada munic√≠pio = 100%)
df_tend_percent = df_tend.div(df_tend.sum(axis=1), axis=0) * 100

# Cria cluster heatmap
sns.clustermap(
    df_tend_percent,
    cmap="YlGnBu",
    figsize=(12,10),
    annot=True, fmt=".1f",
    cbar_kws={'label': 'Propor√ß√£o (%)'},
    metric="euclidean",  # dist√¢ncia usada para agrupar
    method="ward"        # m√©todo de liga√ß√£o hier√°rquica
)

plt.suptitle("Cluster Heatmap das tend√™ncias Mann-Kendall por munic√≠pio", y=1.02)
plt.show()

# Cluster Heatmap por tipo de s√©rie
# Vers√£o clusterizada separada por tipo de s√©rie (chuva, vaz√£o, n√≠vel).
import seaborn as sns
import matplotlib.pyplot as plt

# Loop para cada tipo de s√©rie
for tipo in df_mapa["tipo"].unique():
    # Filtra apenas o tipo atual
    df_tipo = df_mapa[df_mapa["tipo"] == tipo]

    # Conta tend√™ncias por munic√≠pio
    df_tend = df_tipo.groupby(["City", "tendencia_MK"]).size().unstack(fill_value=0)

    # Normaliza para porcentagem
    df_tend_percent = df_tend.div(df_tend.sum(axis=1), axis=0) * 100

    # Cria cluster heatmap
    sns.clustermap(
        df_tend_percent,
        cmap="YlGnBu",
        figsize=(12,10),
        annot=True, fmt=".1f",
        cbar_kws={'label': 'Propor√ß√£o (%)'},
        metric="euclidean",
        method="ward"
    )

    plt.suptitle(f"Cluster Heatmap das tend√™ncias Mann-Kendall por munic√≠pio ({tipo})", y=1.02)
    plt.show()

# Cluster Heatmap comparativo (chuva, vaz√£o e n√≠vel)
import seaborn as sns
import matplotlib.pyplot as plt

# Define os tipos de s√©rie
tipos = df_mapa["tipo"].unique()

# Cria subplots lado a lado
fig, axes = plt.subplots(1, len(tipos), figsize=(18,8))

for ax, tipo in zip(axes, tipos):
    # Filtra apenas o tipo atual
    df_tipo = df_mapa[df_mapa["tipo"] == tipo]

    # Conta tend√™ncias por munic√≠pio
    df_tend = df_tipo.groupby(["City", "tendencia_MK"]).size().unstack(fill_value=0)

    # Normaliza para porcentagem
    df_tend_percent = df_tend.div(df_tend.sum(axis=1), axis=0) * 100

    # Heatmap simples (sem dendrograma, para caber nos subplots)
    sns.heatmap(
        df_tend_percent,
        cmap="YlGnBu",
        annot=True, fmt=".1f",
        cbar_kws={'label': 'Propor√ß√£o (%)'},
        ax=ax
    )

    ax.set_title(f"Tend√™ncias ({tipo})")
    ax.set_xlabel("Tend√™ncia")
    ax.set_ylabel("Munic√≠pio")

plt.suptitle("Compara√ß√£o das tend√™ncias Mann-Kendall por munic√≠pio (chuva, vaz√£o e n√≠vel)", y=1.02)
plt.tight_layout()
plt.show()

# Cluster Heatmap com dendrograma para cada tipo de s√©rie
import seaborn as sns
import matplotlib.pyplot as plt

# Define os tipos de s√©rie
tipos = df_mapa["tipo"].unique()

for tipo in tipos:
    # Filtra apenas o tipo atual
    df_tipo = df_mapa[df_mapa["tipo"] == tipo]

    # Conta tend√™ncias por munic√≠pio
    df_tend = df_tipo.groupby(["City", "tendencia_MK"]).size().unstack(fill_value=0)

    # Normaliza para porcentagem
    df_tend_percent = df_tend.div(df_tend.sum(axis=1), axis=0) * 100

    # Cria cluster heatmap com dendrograma
    g = sns.clustermap(
        df_tend_percent,
        cmap="YlGnBu",
        figsize=(12,10),
        annot=True, fmt=".1f",
        cbar_kws={'label': 'Propor√ß√£o (%)'},
        metric="euclidean",
        method="ward"
    )

    # Ajusta t√≠tulo
    plt.suptitle(f"Cluster Heatmap das tend√™ncias Mann-Kendall por munic√≠pio ({tipo})", y=1.02)
    plt.show()

# Cluster Heatmap com dendrograma e exporta√ß√£o autom√°tica
import seaborn as sns
import matplotlib.pyplot as plt

# Define os tipos de s√©rie
tipos = df_mapa["tipo"].unique()

for tipo in tipos:
    # Filtra apenas o tipo atual
    df_tipo = df_mapa[df_mapa["tipo"] == tipo]

    # Conta tend√™ncias por munic√≠pio
    df_tend = df_tipo.groupby(["City", "tendencia_MK"]).size().unstack(fill_value=0)

    # Normaliza para porcentagem
    df_tend_percent = df_tend.div(df_tend.sum(axis=1), axis=0) * 100

    # Cria cluster heatmap com dendrograma
    g = sns.clustermap(
        df_tend_percent,
        cmap="YlGnBu",
        figsize=(12,10),
        annot=True, fmt=".1f",
        cbar_kws={'label': 'Propor√ß√£o (%)'},
        metric="euclidean",
        method="ward"
    )

    # Ajusta t√≠tulo
    plt.suptitle(f"Cluster Heatmap das tend√™ncias Mann-Kendall por munic√≠pio ({tipo})", y=1.02)

    # Salva gr√°fico em PNG
    g.savefig(f"cluster_{tipo}.png")

    # Mostra gr√°fico
    plt.show()

# Cluster Heatmap com exporta√ß√£o para PNG e PDF
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages

# Define os tipos de s√©rie
tipos = df_mapa["tipo"].unique()

# Cria um PDF para salvar todos os gr√°ficos juntos
with PdfPages("cluster_tendencias.pdf") as pdf:
    for tipo in tipos:
        # Filtra apenas o tipo atual
        df_tipo = df_mapa[df_mapa["tipo"] == tipo]

        # Conta tend√™ncias por munic√≠pio
        df_tend = df_tipo.groupby(["City", "tendencia_MK"]).size().unstack(fill_value=0)

        # Normaliza para porcentagem
        df_tend_percent = df_tend.div(df_tend.sum(axis=1), axis=0) * 100

        # Cria cluster heatmap com dendrograma
        g = sns.clustermap(
            df_tend_percent,
            cmap="YlGnBu",
            figsize=(12,10),
            annot=True, fmt=".1f",
            cbar_kws={'label': 'Propor√ß√£o (%)'},
            metric="euclidean",
            method="ward"
        )

        # Ajusta t√≠tulo
        plt.suptitle(f"Cluster Heatmap das tend√™ncias Mann-Kendall por munic√≠pio ({tipo})", y=1.02)

        # Salva gr√°fico em PNG
        g.savefig(f"cluster_{tipo}.png")

        # Salva tamb√©m no PDF
        pdf.savefig(g.fig)

        # Mostra gr√°fico
        plt.show()

# Diagn√≥stico - quais c√≥digos n√£o est√£o sendo associados a munic√≠pios e por isso n√£o aparecem nos gr√°ficos.

# Diagn√≥stico: quantos munic√≠pios √∫nicos apareceram
print("Munic√≠pios √∫nicos no df_mapa:", df_mapa["City"].nunique())

# Lista todos os munic√≠pios encontrados
print("Lista de munic√≠pios:", df_mapa["City"].unique())

# Verifica se h√° c√≥digos sem correspond√™ncia
df_sem_city = df_mapa[df_mapa["City"].isna()]
print("C√≥digos sem munic√≠pio:", df_sem_city["codigo"].unique())

# Corre√ß√£o: padroniza c√≥digos e preenche valores nulos
inv_total["Code"] = inv_total["Code"].astype(str).str.strip().str.zfill(5)
df_resumo["codigo"] = df_resumo["codigo"].astype(str).str.strip().str.zfill(5)

# Atualiza df_mapa com merge corrigido
df_mapa = df_resumo.merge(inv_total, left_on="codigo", right_on="Code", how="left")

# Preenche munic√≠pios faltantes com 'Desconhecido'
df_mapa["City"] = df_mapa["City"].fillna("Desconhecido")

# Verifica novamente
print("Munic√≠pios √∫nicos ap√≥s corre√ß√£o:", df_mapa["City"].nunique())

# Tabela resumo: n√∫mero de s√©ries por munic√≠pio
tabela_resumo = (
    df_mapa.groupby("City")["codigo"]
    .count()
    .reset_index()
    .rename(columns={"codigo": "num_series"})
    .sort_values(by="num_series", ascending=False)
)

print(tabela_resumo)

# Se quiser visualizar como gr√°fico de barras horizontal
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
sns.barplot(
    data=tabela_resumo,
    y="City",
    x="num_series",
    orient="h",
    palette="viridis"
)
plt.title("N√∫mero de s√©ries por munic√≠pio")
plt.xlabel("N√∫mero de s√©ries")
plt.ylabel("Munic√≠pio")
plt.show()

# Lambari D‚ÄôOeste, Nossa Senhora do Livramento, Santo Ant√¥nio do Leverger, Bodoquena, Rio Verde de Mato Grosso
# podem estar faltando porque o nome no invent√°rio n√£o bate exatamente com o que voc√™ est√° usando no `df_mapa`.
# Busca por parte do nome
# Lista de munic√≠pios que voc√™ quer garantir
municipios_pantanal = [
    "LAMBARI D‚ÄôOESTE",
    "NOSSA SENHORA DO LIVRAMENTO",
    "SANTO ANT√îNIO DO LEVERGER",
    "BODOQUENA",
    "RIO VERDE DE MATO GROSSO"
]

# Busca por substring (case insensitive)
for mun in municipios_pantanal:
    encontrados = inv_total[inv_total["City"].str.contains(mun.split()[0], case=False, na=False)]
    print(f"\nBusca por '{mun}':")
    print(encontrados[["Code","City"]])

!pip install fuzzywuzzy[speedup]

# Exemplo com fuzzy match (mais flex√≠vel)
# Retorna o munic√≠pio mais parecido no invent√°rio, mesmo que o nome esteja escrito de forma diferente.

from fuzzywuzzy import process

# Lista de todos os munic√≠pios do invent√°rio
municipios_lista = inv_total["City"].dropna().unique()

for mun in municipios_pantanal:
    match = process.extractOne(mun, municipios_lista)
    print(f"{mun} -> {match}")

#Substitui√ß√£o
df_mapa["City"] = df_mapa["City"].replace({
    "Lambari D'Oeste": "LAMBARI D‚ÄôOESTE",
    "N. Sra do Livramento": "NOSSA SENHORA DO LIVRAMENTO",
    "Santo Antonio do Leverger": "SANTO ANT√îNIO DO LEVERGER",
    "Bodoquena": "BODOQUENA",
    "Rio Verde": "RIO VERDE DE MATO GROSSO"
})

# Busca por partes dos nomes
municipios_pantanal = [
    "LAMBARI",
    "LIVRAMENTO",
    "LEVERGER",
    "BODOQUENA",
    "RIO VERDE"
]

for mun in municipios_pantanal:
    encontrados = inv_total[inv_total["City"].str.contains(mun, case=False, na=False)]
    print(f"\nBusca por '{mun}':")
    print(encontrados[["Code","City"]])

# script autom√°tico que busca os munic√≠pios faltantes por parte do nome e j√° sugere substitui√ß√µes para padronizar.
# Assim voc√™ garante que todos os 16 munic√≠pios apare√ßam corretamente no df_mapa.
# Lista de munic√≠pios que voc√™ quer garantir LAMBARI D‚ÄôOESTE, NOSSA SENHORA DO LIVRAMENTO,SANTO ANT√îNIO DO LEVERGER,BODOQUENA e RIO VERDE DE MATO GROSSO

municipios_pantanal = [
    "BAR√ÉO DE MELGA√áO","C√ÅCERES","LAMBARI D‚ÄôOESTE","POCON√â",
    "NOSSA SENHORA DO LIVRAMENTO","SANTO ANT√îNIO DO LEVERGER","ITIQUIRA",
    "CORUMB√Å","LAD√ÅRIO","AQUIDAUANA","MIRANDA","BODOQUENA",
    "PORTO MURTINHO","RIO VERDE DE MATO GROSSO","SONORA","COXIM"
]

# Dicion√°rio para armazenar correspond√™ncias encontradas
substituicoes = {}

# Busca por parte do nome (case insensitive)
for mun in municipios_pantanal:
    encontrados = inv_total[inv_total["City"].str.contains(mun.split()[0], case=False, na=False)]
    print(f"\nBusca por '{mun}':")
    print(encontrados[["Code","City"]])

    # Se encontrou algo, pega o primeiro nome e sugere substitui√ß√£o
    if not encontrados.empty:
        nome_inventario = encontrados.iloc[0]["City"]
        substituicoes[nome_inventario] = mun

# Mostra dicion√°rio de substitui√ß√µes sugerido
print("\nDicion√°rio de substitui√ß√µes sugerido:")
print(substituicoes)

# Aplica substitui√ß√µes no df_mapa
df_mapa["City"] = df_mapa["City"].replace(substituicoes)

# Verifica novamente os munic√≠pios √∫nicos
print("\nMunic√≠pios √∫nicos ap√≥s substitui√ß√£o:", df_mapa["City"].nunique())
print(df_mapa["City"].unique())

# Script completo: busca parcial + substitui√ß√£o + relat√≥rio em CSV

import pandas as pd

# Lista de munic√≠pios que voc√™ quer garantir
municipios_pantanal = [
    "BAR√ÉO DE MELGA√áO","C√ÅCERES","LAMBARI D‚ÄôOESTE","POCON√â",
    "NOSSA SENHORA DO LIVRAMENTO","SANTO ANT√îNIO DO LEVERGER","ITIQUIRA",
    "CORUMB√Å","LAD√ÅRIO","AQUIDAUANA","MIRANDA","BODOQUENA",
    "PORTO MURTINHO","RIO VERDE DE MATO GROSSO","SONORA","COXIM"
]

# Dicion√°rio para armazenar correspond√™ncias encontradas
substituicoes = {}
relatorio = []

# Busca por parte do nome (case insensitive)
for mun in municipios_pantanal:
    encontrados = inv_total[inv_total["City"].str.contains(mun.split()[0], case=False, na=False)]

    if not encontrados.empty:
        # Pega o primeiro resultado como correspond√™ncia
        nome_inventario = encontrados.iloc[0]["City"]
        codigo = encontrados.iloc[0]["Code"]

        # Cria substitui√ß√£o
        substituicoes[nome_inventario] = mun

        # Adiciona ao relat√≥rio
        relatorio.append({
            "Nome invent√°rio": nome_inventario,
            "Nome padronizado": mun,
            "C√≥digo": codigo
        })
    else:
        # Caso n√£o encontre nada
        relatorio.append({
            "Nome invent√°rio": "N√ÉO ENCONTRADO",
            "Nome padronizado": mun,
            "C√≥digo": "-"
        })

# Aplica substitui√ß√µes no df_mapa
df_mapa["City"] = df_mapa["City"].replace(substituicoes)

# Cria DataFrame do relat√≥rio
df_relatorio = pd.DataFrame(relatorio)

# Mostra relat√≥rio
print(df_relatorio)

# Exporta para CSV
df_relatorio.to_csv("relatorio_municipios.csv", index=False, encoding="utf-8-sig")

print("\nRelat√≥rio exportado para 'relatorio_municipios.csv'")

# Script completo: busca parcial + substitui√ß√£o + relat√≥rio com n√∫mero de s√©ries + exporta√ß√£o CSV
import pandas as pd

# Lista de munic√≠pios que voc√™ quer garantir
municipios_pantanal = [
    "BAR√ÉO DE MELGA√áO","C√ÅCERES","LAMBARI D‚ÄôOESTE","POCON√â",
    "NOSSA SENHORA DO LIVRAMENTO","SANTO ANT√îNIO DO LEVERGER","ITIQUIRA",
    "CORUMB√Å","LAD√ÅRIO","AQUIDAUANA","MIRANDA","BODOQUENA",
    "PORTO MURTINHO","RIO VERDE DE MATO GROSSO","SONORA","COXIM"
]

# Dicion√°rio para armazenar correspond√™ncias encontradas
substituicoes = {}
relatorio = []

# Busca por parte do nome (case insensitive)
for mun in municipios_pantanal:
    encontrados = inv_total[inv_total["City"].str.contains(mun.split()[0], case=False, na=False)]

    if not encontrados.empty:
        # Pega o primeiro resultado como correspond√™ncia
        nome_inventario = encontrados.iloc[0]["City"]
        codigo = encontrados.iloc[0]["Code"]

        # Cria substitui√ß√£o
        substituicoes[nome_inventario] = mun

        # Conta quantas s√©ries esse munic√≠pio tem no df_mapa
        num_series = df_mapa[df_mapa["City"] == nome_inventario].shape[0]

        # Adiciona ao relat√≥rio
        relatorio.append({
            "Nome invent√°rio": nome_inventario,
            "Nome padronizado": mun,
            "C√≥digo": codigo,
            "N√∫mero de s√©ries": num_series
        })
    else:
        # Caso n√£o encontre nada
        relatorio.append({
            "Nome invent√°rio": "N√ÉO ENCONTRADO",
            "Nome padronizado": mun,
            "C√≥digo": "-",
            "N√∫mero de s√©ries": 0
        })

# Aplica substitui√ß√µes no df_mapa
df_mapa["City"] = df_mapa["City"].replace(substituicoes)

# Cria DataFrame do relat√≥rio
df_relatorio = pd.DataFrame(relatorio)

# Mostra relat√≥rio
print(df_relatorio)

# Exporta para CSV
df_relatorio.to_csv("relatorio_municipios_num_serie.csv", index=False, encoding="utf-8-sig")

print("\nRelat√≥rio exportado para 'relatorio_municipios_num_serie.csv'")

# Interpola√ß√£o Temporal
import pandas as pd
import numpy as np

# Criando uma s√©rie de exemplo com datas faltantes
datas = pd.date_range("2020-01-01", "2020-01-10", freq="D")
valores = [10, np.nan, 12, np.nan, 15, 16, np.nan, 18, 19, np.nan]

df = pd.DataFrame({"nivel": valores}, index=datas)

print("üìâ S√©rie original com valores faltantes:")
print(df)

# Fun√ß√£o de interpola√ß√£o temporal
def interpolar_temporal(df, coluna_valor="nivel", freq="D", metodo="linear"):
    """
    Interpola√ß√£o temporal para s√©ries hidrol√≥gicas.
    """
    df.index = pd.to_datetime(df.index)
    df_resampled = df.resample(freq).mean()
    df_resampled[coluna_valor] = df_resampled[coluna_valor].interpolate(method=metodo)
    df_resampled["interpolado"] = df[coluna_valor].isna().reindex(df_resampled.index, fill_value=False)
    return df_resampled

# Aplicando a fun√ß√£o
df_interpolado = interpolar_temporal(df, coluna_valor="nivel", freq="D", metodo="linear")

print("\n‚úÖ S√©rie interpolada:")
print(df_interpolado)

# C√≥digo com gr√°fico comparativo

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Criando uma s√©rie de exemplo com datas faltantes
datas = pd.date_range("2020-01-01", "2020-01-10", freq="D")
valores = [10, np.nan, 12, np.nan, 15, 16, np.nan, 18, 19, np.nan]

df = pd.DataFrame({"nivel": valores}, index=datas)

# Fun√ß√£o de interpola√ß√£o temporal
def interpolar_temporal(df, coluna_valor="nivel", freq="D", metodo="linear"):
    df.index = pd.to_datetime(df.index)
    df_resampled = df.resample(freq).mean()
    df_resampled[coluna_valor] = df_resampled[coluna_valor].interpolate(method=metodo)
    df_resampled["interpolado"] = df[coluna_valor].isna().reindex(df_resampled.index, fill_value=False)
    return df_resampled

# Aplicando a fun√ß√£o
df_interpolado = interpolar_temporal(df, coluna_valor="nivel", freq="D", metodo="linear")

# üîπ Gr√°fico comparativo
plt.figure(figsize=(10,5))
plt.plot(df.index, df["nivel"], "o-", label="Original (com NaN)", color="blue")
plt.plot(df_interpolado.index, df_interpolado["nivel"], "o-", label="Interpolada", color="orange")

# Destacar pontos interpolados
plt.scatter(df_interpolado.index[df_interpolado["interpolado"]],
            df_interpolado["nivel"][df_interpolado["interpolado"]],
            color="red", marker="x", s=100, label="Valores interpolados")

plt.title("Interpola√ß√£o Temporal - S√©rie Hidrol√≥gica")
plt.xlabel("Data")
plt.ylabel("N√≠vel")
plt.legend()
plt.grid(True)
plt.show()

# C√≥digo comparativo de m√©todos de interpola√ß√£o
# Deixar o c√≥digo comparativo funcionando com linear, nearest e polynomial (order=2):

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Criando uma s√©rie de exemplo com datas faltantes
datas = pd.date_range("2020-01-01", "2020-01-10", freq="D")
valores = [10, np.nan, 12, np.nan, 15, 16, np.nan, 18, 19, np.nan]
df = pd.DataFrame({"nivel": valores}, index=datas)

# Fun√ß√£o de interpola√ß√£o temporal
def interpolar_temporal(df, coluna_valor="nivel", freq="D", metodo="linear", order=None):
    df.index = pd.to_datetime(df.index)
    df_resampled = df.resample(freq).mean()
    if order:
        df_resampled[coluna_valor] = df_resampled[coluna_valor].interpolate(method=metodo, order=order)
    else:
        df_resampled[coluna_valor] = df_resampled[coluna_valor].interpolate(method=metodo)
    return df_resampled

# Aplicando diferentes m√©todos
df_linear = interpolar_temporal(df, metodo="linear")
df_nearest = interpolar_temporal(df, metodo="nearest")
df_polynomial = interpolar_temporal(df, metodo="polynomial", order=2)  # ordem polinomial definida

# Gr√°fico comparativo
plt.figure(figsize=(12,6))

plt.plot(df.index, df["nivel"], "o-", label="Original (com NaN)", color="blue")
plt.plot(df_linear.index, df_linear["nivel"], "o-", label="Linear", color="orange")
plt.plot(df_nearest.index, df_nearest["nivel"], "o-", label="Nearest", color="green")
plt.plot(df_polynomial.index, df_polynomial["nivel"], "o-", label="Polynomial (ordem=2)", color="red")

plt.title("Compara√ß√£o de M√©todos de Interpola√ß√£o")
plt.xlabel("Data")
plt.ylabel("N√≠vel")
plt.legend()
plt.grid(True)
plt.show()

# C√≥digo com RMSE e MAE
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Criando s√©rie de exemplo com valores faltantes
datas = pd.date_range("2020-01-01", "2020-01-10", freq="D")
valores = [10, np.nan, 12, np.nan, 15, 16, np.nan, 18, 19, np.nan]
df = pd.DataFrame({"nivel": valores}, index=datas)

# Fun√ß√£o de interpola√ß√£o temporal
def interpolar_temporal(df, coluna_valor="nivel", freq="D", metodo="linear", order=None):
    df.index = pd.to_datetime(df.index)
    df_resampled = df.resample(freq).mean()
    if order:
        df_resampled[coluna_valor] = df_resampled[coluna_valor].interpolate(method=metodo, order=order)
    else:
        df_resampled[coluna_valor] = df_resampled[coluna_valor].interpolate(method=metodo)
    return df_resampled

# Aplicando diferentes m√©todos
df_linear = interpolar_temporal(df, metodo="linear")
df_nearest = interpolar_temporal(df, metodo="nearest")
df_polynomial = interpolar_temporal(df, metodo="polynomial", order=2)

#  Gr√°fico comparativo
plt.figure(figsize=(12,6))
plt.plot(df.index, df["nivel"], "o-", label="Original (com NaN)", color="blue")
plt.plot(df_linear.index, df_linear["nivel"], "o-", label="Linear", color="orange")
plt.plot(df_nearest.index, df_nearest["nivel"], "o-", label="Nearest", color="green")
plt.plot(df_polynomial.index, df_polynomial["nivel"], "o-", label="Polynomial (ordem=2)", color="red")
plt.title("Compara√ß√£o de M√©todos de Interpola√ß√£o")
plt.xlabel("Data")
plt.ylabel("N√≠vel")
plt.legend()
plt.grid(True)
plt.show()

# Avalia√ß√£o quantitativa
# Usamos apenas os pontos originais n√£o nulos como refer√™ncia
mask = ~df["nivel"].isna()
y_true = df["nivel"][mask]

# Extra√≠mos os valores interpolados correspondentes
y_linear = df_linear["nivel"][mask]
y_nearest = df_nearest["nivel"][mask]
y_poly = df_polynomial["nivel"][mask]

# Calculamos m√©tricas
def avaliar(y_true, y_pred, metodo):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    print(f"{metodo}: RMSE={rmse:.3f}, MAE={mae:.3f}")

avaliar(y_true, y_linear, "Linear")
avaliar(y_true, y_nearest, "Nearest")
avaliar(y_true, y_poly, "Polynomial (ordem=2)")

#C√≥digo para m√∫ltiplas esta√ß√µes

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error
import os

# Fun√ß√£o de interpola√ß√£o temporal
def interpolar_temporal(df, coluna_valor="nivel", freq="D", metodo="linear", order=None):
    df.index = pd.to_datetime(df.index)
    df_resampled = df.resample(freq).mean()
    if order:
        df_resampled[coluna_valor] = df_resampled[coluna_valor].interpolate(method=metodo, order=order)
    else:
        df_resampled[coluna_valor] = df_resampled[coluna_valor].interpolate(method=metodo)
    return df_resampled

# Fun√ß√£o para avaliar interpola√ß√£o
def avaliar(y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    return rmse, mae

#  Pipeline para v√°rias esta√ß√µes
def processar_estacoes(dados_estacoes, coluna_valor="nivel", freq="D"):
    os.makedirs("results/interpolacao", exist_ok=True)
    resultados = []

    for nome, df in dados_estacoes.items():
        print(f"Processando esta√ß√£o: {nome}")

        # Aplicar m√©todos
        df_linear = interpolar_temporal(df, coluna_valor=coluna_valor, freq=freq, metodo="linear")
        df_nearest = interpolar_temporal(df, coluna_valor=coluna_valor, freq=freq, metodo="nearest")
        df_poly = interpolar_temporal(df, coluna_valor=coluna_valor, freq=freq, metodo="polynomial", order=2)

        # Gr√°fico comparativo
        plt.figure(figsize=(12,6))
        plt.plot(df.index, df[coluna_valor], "o-", label="Original", color="blue")
        plt.plot(df_linear.index, df_linear[coluna_valor], "o-", label="Linear", color="orange")
        plt.plot(df_nearest.index, df_nearest[coluna_valor], "o-", label="Nearest", color="green")
        plt.plot(df_poly.index, df_poly[coluna_valor], "o-", label="Polynomial (ordem=2)", color="red")
        plt.title(f"Interpola√ß√£o Temporal - Esta√ß√£o {nome}")
        plt.xlabel("Data")
        plt.ylabel(coluna_valor)
        plt.legend()
        plt.grid(True)
        plt.savefig(f"results/interpolacao/{nome}_comparacao.png")
        plt.close()

        # Avalia√ß√£o quantitativa (apenas pontos originais n√£o nulos)
        mask = ~df[coluna_valor].isna()
        y_true = df[coluna_valor][mask]
        y_linear = df_linear[coluna_valor][mask]
        y_nearest = df_nearest[coluna_valor][mask]
        y_poly = df_poly[coluna_valor][mask]

        resultados.append({
            "estacao": nome,
            "RMSE_linear": avaliar(y_true, y_linear)[0],
            "MAE_linear": avaliar(y_true, y_linear)[1],
            "RMSE_nearest": avaliar(y_true, y_nearest)[0],
            "MAE_nearest": avaliar(y_true, y_nearest)[1],
            "RMSE_poly": avaliar(y_true, y_poly)[0],
            "MAE_poly": avaliar(y_true, y_poly)[1],
        })

    # Salvar m√©tricas em CSV
    # Cria DataFrame do relat√≥rio

df_relatorio = pd.DataFrame(relatorio)

# Mostra relat√≥rio
print(df_relatorio)

# Exporta para CSV
df_relatorio.to_csv("metricas_interpolacao.csv", index=False, encoding="utf-8-sig")

print("\‚úÖ Processamento conclu√≠do dos Gr√°ficos e m√©tricas salvos.")

# Exemplo de dados fict√≠cios para duas esta√ß√µes
datas = pd.date_range("2020-01-01", "2020-01-10", freq="D")
valores1 = [10, np.nan, 12, np.nan, 15, 16, np.nan, 18, 19, np.nan]
valores2 = [20, 21, np.nan, 23, np.nan, 25, 26, np.nan, 28, 29]

dados_estacoes = {
    "Estacao_MT_01": pd.DataFrame({"nivel": valores1}, index=datas),
    "Estacao_MS_02": pd.DataFrame({"nivel": valores2}, index=datas)
}

processar_estacoes(dados_estacoes, coluna_valor="nivel", freq="D")

# Leitura do XML https://telemetriaws1.ana.gov.br/ServiceANA.asmx?op=ListaEstacoesTelemetricas

import pandas as pd

# URL do XML no site da ANA
url = "https://telemetriaws1.ana.gov.br/ServiceANA.asmx?op=ListaEstacoesTelemetricas"

# Ler direto do site
df = pd.read_xml(url)

# Separar munic√≠pio e UF
df[["Municipio", "UF"]] = df["Municipio-UF"].str.split("-", expand=True)

# Selecionar colunas de interesse
df_lista = df[["CodEstacao", "NomeEstacao", "Municipio", "UF", "NomeRio"]]

# Salvar em CSV
df_lista.to_csv("lista_estacoes.csv", index=False)

print("‚úÖ Lista salva em 'lista_estacoes.csv'")

##  Passo 1: Ler o TXT
# Mesmo que o arquivo seja XML salvo como `.txt`, o `pandas` consegue interpretar:


import pandas as pd

# Ler o TXT que cont√©m XML
df = pd.read_xml("ListaEstacoesTelemetricas_MS_MT.txt")

# Separar munic√≠pio e UF
df[["Municipio", "UF"]] = df["Municipio-UF"].str.split("-", expand=True)

# Filtrar apenas MT e MS
df_mtms = df[df["UF"].isin(["MT", "MS"])]

# Visualizar primeiras linhas
print(df_mtms.head())

#  Script completo

import pandas as pd

# 1. Ler apenas os elementos <Table> dentro do XML
df = pd.read_xml("ListaEstacoesTelemetricas_MS_MT_v2.txt", xpath="//Table")

# 2. Separar Munic√≠pio e UF
df[["Municipio", "UF"]] = df["Municipio-UF"].str.split("-", expand=True)

# 3. Filtrar apenas MT e MS
df_mtms = df[df["UF"].isin(["MT", "MS"])]

# 4. Reorganizar colunas para facilitar leitura
df_mtms = df_mtms[[
    "CodEstacao",
    "NomeEstacao",
    "Municipio",
    "UF",
    "NomeRio",
    "CodRio",
    "Operadora",
    "Responsavel",
    "Latitude",
    "Longitude",
    "Altitude",
    "Bacia",
    "SubBacia",
    "Origem",
    "StatusEstacao"
]]

# 5. Exportar para Excel
df_mtms.to_excel("Estacoes_MT_MS.xlsx", index=False)

print("Arquivo 'Estacoes_MT_MS.xlsx' gerado com sucesso!")

### Transpor com Pandas Inverter linhas ‚Üî colunas:


# Transpor o DataFrame filtrado
df_transposto = df_mtms.T

# Exportar para Excel
df_transposto.to_excel("Estacoes_MT_MS_transposto.xlsx", index=True)

import pandas as pd

# 1. Ler apenas os elementos <Table> dentro do XML
df = pd.read_xml("ListaEstacoesTelemetricas_MS_MT_v2.txt", xpath="//Table")

# 2. Separar Munic√≠pio e UF
df[["Municipio", "UF"]] = df["Municipio-UF"].str.split("-", expand=True)

# 3. Filtrar apenas MT e MS
df_mtms = df[df["UF"].isin(["MT", "MS"])]

# 4. Reorganizar colunas para facilitar leitura
df_mtms = df_mtms[[
    "CodEstacao",
    "NomeEstacao",
    "Municipio",
    "UF",
    "NomeRio",
    "CodRio",
    "Operadora",
    "Responsavel",
    "Latitude",
    "Longitude",
    "Altitude",
    "Bacia",
    "SubBacia",
    "Origem",
    "StatusEstacao"
]]

# 5. Exportar vers√£o normal para Excel
df_mtms.to_excel("Estacoes_MT_MS.xlsx", index=False)

# 6. Exportar vers√£o transposta (linhas viram colunas)
df_transposto = df_mtms.T
df_transposto.to_excel("Estacoes_MT_MS_transposto.xlsx", index=True)

print("Arquivos 'Estacoes_MT_MS.xlsx' e 'Estacoes_MT_MS_transposto.xlsx' gerados com sucesso!")

# `XMLSyntaxError: StartTag: invalid element name` acontece porque o arquivo `.txt`
# n√£o est√° formatado como um XML completo ‚Äî ele provavelmente come√ßa direto com `<Table>`
# sem um cabe√ßalho ou uma raiz √∫nica. O `pandas.read_xml` exige que o conte√∫do seja um XML bem formado.

# Como resolver
# Ler como texto e usar `lxml` ou `BeautifulSoup`

import pandas as pd

# Ler o arquivo como texto
with open("ListaEstacoesTelemetricas_MS_MT_v2.txt", "r", encoding="utf-8") as f:
    xml_content = f.read()

# Adicionar uma raiz artificial
xml_content = "<Root>" + xml_content + "</Root>"

# Ler com pandas
df = pd.read_xml(xml_content, xpath="//Table")

# Separar Munic√≠pio e UF
df[["Municipio", "UF"]] = df["Municipio-UF"].str.split("-", expand=True)

# Filtrar apenas MT e MS
df_mtms = df[df["UF"].isin(["MT", "MS"])]

# Reorganizar colunas
df_mtms = df_mtms[[
    "CodEstacao",
    "NomeEstacao",
    "Municipio",
    "UF",
    "NomeRio",
    "CodRio",
    "Operadora",
    "Responsavel",
    "Latitude",
    "Longitude",
    "Altitude",
    "Bacia",
    "SubBacia",
    "Origem",
    "StatusEstacao"
]]

# Exportar vers√£o normal
df_mtms.to_excel("Estacoes_MT_MS.xlsx", index=False)

# Exportar vers√£o transposta
df_mtms.T.to_excel("Estacoes_MT_MS_transposto.xlsx", index=True)

print("Arquivos gerados com sucesso!")

import pandas as pd
from io import StringIO

# 1. Ler o arquivo como texto
with open("ListaEstacoesTelemetricas_MS_MT_v2.txt", "r", encoding="utf-8") as f:
    xml_content = f.read()

# 2. Adicionar uma raiz artificial <Root> ... </Root>
xml_content = "<Root>" + xml_content + "</Root>"

# 3. Usar StringIO para passar a string como "arquivo"
df = pd.read_xml(StringIO(xml_content), xpath="//Table")

# 4. Separar Munic√≠pio e UF
df[["Municipio", "UF"]] = df["Municipio-UF"].str.split("-", expand=True)

# 5. Filtrar apenas MT e MS
df_mtms = df[df["UF"].isin(["MT", "MS"])]

# 6. Reorganizar colunas
df_mtms = df_mtms[[
    "CodEstacao",
    "NomeEstacao",
    "Municipio",
    "UF",
    "NomeRio",
    "CodRio",
    "Operadora",
    "Responsavel",
    "Latitude",
    "Longitude",
    "Altitude",
    "Bacia",
    "SubBacia",
    "Origem",
    "StatusEstacao"
]]

# 7. Exportar vers√£o normal
df_mtms.to_excel("Estacoes_MT_MS.xlsx", index=False)

# 8. Exportar vers√£o transposta
df_mtms.T.to_excel("Estacoes_MT_MS_transposto.xlsx", index=True)

print("Arquivos 'Estacoes_MT_MS.xlsx' e 'Estacoes_MT_MS_transposto.xlsx' gerados com sucesso!")

# Solu√ß√£o pr√°tica
# Em vez de tentar for√ßar o read_xml, voc√™ pode usar o BeautifulSoup ou o lxml.etree para tratar o conte√∫do como XML parcial
# e depois converter para DataFrame.

import pandas as pd
from bs4 import BeautifulSoup

# 1. Ler o arquivo como texto
with open("ListaEstacoesTelemetricas_MS_MT_v2.txt", "r", encoding="utf-8") as f:
    xml_content = f.read()

# 2. Adicionar raiz artificial
xml_content = "<Root>" + xml_content + "</Root>"

# 3. Parsear com BeautifulSoup
soup = BeautifulSoup(xml_content, "xml")

# 4. Extrair todas as tags <Table>
tables = soup.find_all("Table")

# 5. Converter cada <Table> em dicion√°rio
data = []
for t in tables:
    row = {}
    for child in t.find_all():
        row[child.name] = child.text.strip()
    data.append(row)

# 6. Criar DataFrame
df = pd.DataFrame(data)

# 7. Separar Munic√≠pio e UF
df[["Municipio", "UF"]] = df["Municipio-UF"].str.split("-", expand=True)

# 8. Filtrar apenas MT e MS
df_mtms = df[df["UF"].isin(["MT", "MS"])]

# 9. Reorganizar colunas
df_mtms = df_mtms[[
    "CodEstacao",
    "NomeEstacao",
    "Municipio",
    "UF",
    "NomeRio",
    "CodRio",
    "Operadora",
    "Responsavel",
    "Latitude",
    "Longitude",
    "Altitude",
    "Bacia",
    "SubBacia",
    "Origem",
    "StatusEstacao"
]]

# 10. Exportar vers√£o normal
df_mtms.to_excel("Estacoes_MT_MS.xlsx", index=False)

# 11. Exportar vers√£o transposta
df_mtms.T.to_excel("Estacoes_MT_MS_transposto.xlsx", index=True)

print("Arquivos 'Estacoes_MT_MS.xlsx' e 'Estacoes_MT_MS_transposto.xlsx' gerados com sucesso!")

# o erro de `KeyError: 'Municipio-UF'` significa que a coluna **n√£o existe** no `DataFrame` que foi montado.
# Isso acontece porque, ao usar o `BeautifulSoup`, os nomes das tags viram os nomes das colunas ‚Äî e no seu XML a
# tag se chama exatamente `<Municipio-UF>`. O h√≠fen no nome da tag pode estar sendo interpretado de forma diferente ou at√© removido.

import pandas as pd
from bs4 import BeautifulSoup

# 1. Ler o arquivo como texto
with open("ListaEstacoesTelemetricas_MS_MT_v2.txt", "r", encoding="utf-8") as f:
    xml_content = f.read()

# 2. Adicionar raiz artificial
xml_content = "<Root>" + xml_content + "</Root>"

# 3. Parsear com BeautifulSoup
soup = BeautifulSoup(xml_content, "xml")

# 4. Extrair todas as tags <Table>
tables = soup.find_all("Table")

# 5. Converter cada <Table> em dicion√°rio
data = []
for t in tables:
    row = {}
    for child in t.find_all():
        row[child.name] = child.text.strip()
    data.append(row)

# 6. Criar DataFrame
df = pd.DataFrame(data)

# üëâ Ver quais colunas realmente existem
print("Colunas dispon√≠veis:", df.columns.tolist())

# 7. Se a coluna veio com nome diferente (ex: 'Municipio_UF' ou 'MunicipioUF'), renomear:
df.rename(columns={"Municipio_UF": "Municipio-UF", "MunicipioUF": "Municipio-UF"}, inplace=True)

# 8. Agora separar Munic√≠pio e UF
if "Municipio-UF" in df.columns:
    df[["Municipio", "UF"]] = df["Municipio-UF"].str.split("-", expand=True)
else:
    print("‚ö†Ô∏è A coluna 'Municipio-UF' n√£o foi encontrada. Veja os nomes listados acima.")

# 9. Filtrar apenas MT e MS
if "UF" in df.columns:
    df_mtms = df[df["UF"].isin(["MT", "MS"])]
else:
    df_mtms = df

# 10. Reorganizar colunas
colunas = [
    "CodEstacao","NomeEstacao","Municipio","UF","NomeRio","CodRio",
    "Operadora","Responsavel","Latitude","Longitude","Altitude",
    "Bacia","SubBacia","Origem","StatusEstacao"
]
df_mtms = df_mtms[[c for c in colunas if c in df_mtms.columns]]

# 11. Exportar vers√£o normal
df_mtms.to_excel("Estacoes_MT_MS.xlsx", index=False)

# 12. Exportar vers√£o transposta
df_mtms.T.to_excel("Estacoes_MT_MS_transposto.xlsx", index=True)

print("Arquivos gerados com sucesso!")

print("Colunas dispon√≠veis:", df.columns.tolist())